{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "## Starting URL: https://transcripts.cnn.com/\n",
        "\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import re\n",
        "import time\n",
        "\n",
        "# Set data ranges\n",
        "start_date = datetime.strptime(\"2024-07-21\", \"%Y-%m-%d\")\n",
        "end_date = datetime.strptime(\"2024-07-28\", \"%Y-%m-%d\")\n",
        "\n",
        "date_list = pd.date_range(start_date, end_date).strftime(\"%Y-%m-%d\")\n",
        "\n",
        "for date in date_list:\n",
        "    url = 'https://transcripts.cnn.com/date/' + date\n",
        "    print(url)"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "datalore": {
          "node_id": "hxTNilZW6Vgdh1cqQEVQ98",
          "type": "CODE",
          "hide_input_from_viewers": true,
          "hide_output_from_viewers": true
        },
        "id": "PvZBI4TjgqE8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Inclusion Criteria\n",
        "#Dates: July 21 - July 28, 2024\n",
        "#Content: Any transcript mentioning Kamala Harris\n",
        "\n",
        "for date in date_list:\n",
        "    url = 'https://transcripts.cnn.com/date/' + date\n",
        "    html = requests.get(url)\n",
        "    soupified = BeautifulSoup(html.content, \"html.parser\")\n",
        "    page = soupified.find_all('div', {'class': 'cnnSectBulletItems'})\n",
        "    descriptions = re.findall('a href=\\\".*\\\"', str(page))\n",
        "    for description in descriptions:\n",
        "      if 'Harris' in str(description):\n",
        "        print(description)\n",
        "\n",
        "\n",
        "\n",
        "for date in date_list:\n",
        "    url = 'https://transcripts.cnn.com/date/' + date\n",
        "    html = requests.get(url)\n",
        "    soupified = BeautifulSoup(html.content, \"html.parser\")\n",
        "    page = soupified.find_all('div', {'class': 'cnnSectBulletItems'})\n",
        "    descriptions = re.findall('a href=\\\".*\\\"', str(page))\n",
        "    for description in descriptions:\n",
        "      if 'Harris' in str(description):\n",
        "        links = []\n",
        "        link = re.findall('href=\\\"(.*?)\\\"', str(description))\n",
        "        links.append(link[0])\n",
        "        print(link)"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "datalore": {
          "node_id": "exU6bz3FKbTwz8r09JGMLM",
          "type": "CODE",
          "hide_input_from_viewers": true,
          "hide_output_from_viewers": true
        },
        "id": "W9FjZYF9gqE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Pattern of URLs to loop through\n",
        "#Each day has it’s own webpage that contains links to the transcripts: https://transcripts.cnn.com/date/2024-07-24\n",
        "# Need to loop through days, 2024-07-21 to 2024-07-28\n",
        "# URL pattern, ‘https://transcripts.cnn.com/date/’ + date\n",
        "\n",
        "\n",
        "for date in date_list:\n",
        "  url = 'https://transcripts.cnn.com/date/' + date\n",
        "  html = requests.get(url)\n",
        "  soupified = BeautifulSoup(html.content, \"html.parser\")\n",
        "  page = soupified.find_all('div', {'class': 'cnnSectBulletItems'})\n",
        "  descriptions = re.findall('a href=\\\".*\\\"', str(page))\n",
        "  links = []\n",
        "  for description in descriptions:\n",
        "    if 'Harris' in str(description):\n",
        "      link = re.findall('href=\\\"(.*?)\\\"', str(description))\n",
        "      links.append(link[0])\n",
        "  for link in links:\n",
        "    url = \"https://transcripts.cnn.com/\" + link\n",
        "    html = requests.get(url)\n",
        "    soupified = BeautifulSoup(html.content, \"html.parser\")\n",
        "    show_name = re.findall(\"cnnTransStoryHead\\\">(.*?)</p>\", str(soupified))\n",
        "    description = re.findall(\"cnnTransSubHead\\\">(.*?)</p>\", str(soupified))\n",
        "    air_date = re.findall(\"cnnBodyText\\\">Aired (.*?) -\", str(soupified))\n",
        "    print(show_name[0])\n",
        "    print(description[0])\n",
        "    print(air_date[0])"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "datalore": {
          "node_id": "QWPW6qVNDwByZqRR4QVeGd",
          "type": "CODE",
          "hide_input_from_viewers": true,
          "hide_output_from_viewers": true
        },
        "id": "ENAokXN1gqE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data to collect from each page: Data to collect, Show name, Description, Date, Transcript\n",
        "\n",
        "for date in date_list:\n",
        "  url = 'https://transcripts.cnn.com/date/' + date\n",
        "  html = requests.get(url)\n",
        "  soupified = BeautifulSoup(html.content, \"html.parser\")\n",
        "  page = soupified.find_all('div', {'class': 'cnnSectBulletItems'})\n",
        "  descriptions = re.findall('a href=\\\".*\\\"', str(page))\n",
        "  links = []\n",
        "  for description in descriptions:\n",
        "    if 'Harris' in str(description):\n",
        "      link = re.findall('href=\\\"(.*?)\\\"', str(description))\n",
        "      links.append(link[0])\n",
        "  for link in links:\n",
        "    url = \"https://transcripts.cnn.com/\" + link\n",
        "    html = requests.get(url)\n",
        "    soupified = BeautifulSoup(html.content, \"html.parser\")\n",
        "    show_name = re.findall(\"cnnTransStoryHead\\\">(.*?)</p>\", str(soupified))\n",
        "    description = re.findall(\"cnnTransSubHead\\\">(.*?)</p>\", str(soupified))\n",
        "    air_date = re.findall(\"cnnBodyText\\\">Aired (.*?) -\", str(soupified))\n",
        "    page = soupified.find_all(\"p\", {'class':'cnnBodyText'})\n",
        "    text = re.findall(\"<br/>(.*?)<br/>\", str(page))\n",
        "    text = \"\\n\".join(text)\n",
        "    print(text[0:100])"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "datalore": {
          "node_id": "TeNcD2db7IKf62Y4Uooidm",
          "type": "CODE",
          "hide_input_from_viewers": true,
          "hide_output_from_viewers": true
        },
        "id": "h3Z4yBjigqE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import re\n",
        "import time\n",
        "\n",
        "# Set data ranges\n",
        "start_date = datetime.strptime(\"2024-07-21\", \"%Y-%m-%d\")\n",
        "end_date = datetime.strptime(\"2024-07-28\", \"%Y-%m-%d\")\n",
        "\n",
        "date_list = pd.date_range(start_date, end_date).strftime(\"%Y-%m-%d\")\n",
        "\n",
        "transcripts = []\n",
        "\n",
        "for date in date_list:\n",
        "  url = 'https://transcripts.cnn.com/date/' + date\n",
        "  html = requests.get(url)\n",
        "  soupified = BeautifulSoup(html.content, \"html.parser\")\n",
        "  page = soupified.find_all('div', {'class': 'cnnSectBulletItems'})\n",
        "  descriptions = re.findall('a href=\\\".*\\\"', str(page))\n",
        "  links = []\n",
        "  for description in descriptions:\n",
        "    if 'Harris' in str(description):\n",
        "      link = re.findall('href=\\\"(.*?)\\\"', str(description))\n",
        "      links.append(link[0])\n",
        "  for link in links:\n",
        "    url = \"https://transcripts.cnn.com/\" + link\n",
        "    html = requests.get(url)\n",
        "    soupified = BeautifulSoup(html.content, \"html.parser\")\n",
        "    show_name = re.findall(\"cnnTransStoryHead\\\">(.*?)</p>\", str(soupified))\n",
        "    description = re.findall(\"cnnTransSubHead\\\">(.*?)</p>\", str(soupified))\n",
        "    air_date = re.findall(\"cnnBodyText\\\">Aired (.*?) -\", str(soupified))\n",
        "    page = soupified.find_all(\"p\", {'class':'cnnBodyText'})\n",
        "    text = re.findall(\"<br/>(.*?)<br/>\", str(page))\n",
        "    text = \" \".join(text)\n",
        "    transcript = {'Show_Name': show_name[0], 'Description': description[0], 'Air_Date': air_date[0], 'Transcript': text}\n",
        "    transcripts.append(transcript)\n",
        "\n",
        "df = pd.DataFrame(transcripts)\n",
        "df.to_csv('cnn_crawl.csv', index=False)\n",
        "df.head(20)"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "datalore": {
          "node_id": "8JG0CF3lF2w4iyDcUUAwuT",
          "type": "CODE",
          "hide_input_from_viewers": true,
          "hide_output_from_viewers": true
        },
        "id": "dlhMRoBagqE_"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "python"
    },
    "datalore": {
      "computation_mode": "JUPYTER",
      "package_manager": "pip",
      "base_environment": "minimal",
      "packages": [],
      "report_row_ids": [],
      "version": 3
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}