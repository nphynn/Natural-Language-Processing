{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "## Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "from nltk.lm.preprocessing import flatten\n",
        "from nltk.util import ngrams\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud\n",
        "import unicodedata\n",
        "import stop_words\n",
        "import spacy\n",
        "from spacy.lang.en import stop_words\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "tNV5s4Wn4wcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install stop-words"
      ],
      "metadata": {
        "id": "SFpOAp9N4rXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hMp43vXwF8L"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('redditSubmissions.csv', on_bad_lines='warn')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Check rows & columns\n",
        "nRow, nCol = df.shape\n",
        "print(f'There are {nRow} rows and {nCol} columns')\n",
        "\n",
        "\n",
        "#Missing Values\n",
        "df.isnull().sum().sort_values(ascending=False)\n",
        "\n",
        "# Duplicates\n",
        "df.duplicated().sum()\n",
        "\n",
        "# Unique Value\n",
        "df.nunique().sort_values()"
      ],
      "metadata": {
        "id": "JoT5JDE13yPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Summary Statistics\n",
        "df.head()\n",
        "df.info()\n",
        "df.describe(include=\"all\").T"
      ],
      "metadata": {
        "id": "XwQAfrry4NtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization for Numbers\n",
        "frequency_table = df['title'].value_counts()\n",
        "print(frequency_table)\n",
        "\n",
        "frequency_table1 = df['username'].value_counts()\n",
        "print(frequency_table1)\n",
        "\n",
        "frequency_table2 = df['subreddit'].value_counts()\n",
        "print(frequency_table2)\n",
        "\n",
        "frequency_table3 = df['reddit_id'].value_counts()\n",
        "print(frequency_table3)"
      ],
      "metadata": {
        "id": "D3th3Lgs4jdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Clean data\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "nRow, nCol = df.shape\n",
        "print(f'There are {nRow} rows and {nCol} columns')"
      ],
      "metadata": {
        "id": "EtDUPkJ2RfB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize data and remove stopwords, numbers, and punctuations. Also lower case words\n",
        "df['title'] = df['title'].apply(lambda x: unicodedata.normalize('NFKD', x).encode('ascii', 'ignore').decode('utf-8', 'ignore'))\n",
        "df['title'] = df['title'].str.lower()\n",
        "df['title'] = df['title'].str.replace(r'[^\\w\\s]','', regex = True)\n",
        "df['title'] = df['title'].str.replace('\\d+', '', regex=True)\n",
        "stop_words = stop_words.STOP_WORDS\n",
        "df['title'] = df['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))"
      ],
      "metadata": {
        "id": "YyN46GaL3Zrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Summarize at least one metadata variable.\n",
        "df['subreddit'].value_counts(ascending = False)\n",
        "\n",
        "\n",
        "#Plot the 30 most frequent terms in the text.\n",
        "def to_list(strg_list):\n",
        "  return strg_list.strip(\"[]\").replace(\"'\",\"\").replace('\"',\"\").replace(\",\",\"\").split()\n",
        "\n",
        "df['ngram']=[list(ngrams(to_list(strg_list),3))for strg_list in df['title']]\n",
        "count_ngram= Counter(list(flatten([list_item for list_item in df['ngram']])))\n",
        "print(sorted(list(count_ngram.items())[0:100]))\n",
        "\n",
        "\n",
        "unigrams = df['title'].str.split(expand=True).stack().value_counts()[0:30]\n",
        "unigrams.plot(kind = 'barh')\n",
        "\n",
        "\n",
        "#Create a word cloud of the text.\n",
        "df_reddit = df[df['title'].str.contains(\"downvote\")]\n",
        "df1 = df_reddit['title'].str.cat(sep=' ')\n",
        "\n",
        "wc = WordCloud().generate(df1)\n",
        "plt.imshow(wc)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nCrjj6Qg3rVj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}